# Neural Architecture Search (NAS)
In this assignment, you are required to use an open source AutoML toolkit (NNI) for neural architecture search. You are ask to compare three different NAS methods:
Efficient Neural Architecture Search via Parameter Sharing (ENAS)[1], DARTS: Differentiable Architecture Search[2], and Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild[3].

### References 
[1] Pham, H., Guan, M., Zoph, B., Le, Q. and Dean, J., 2018, July. Efficient neural architecture search via parameters sharing. In International Conference on Machine Learning (pp. 4095-4104). PMLR.
[2] Liu, H., Simonyan, K. and Yang, Y., 2018. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055.
[3] Chen, X., Xie, L., Wu, J. and Tian, Q., 2019. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1294-1303).

# Your tasks
- [ ] Installing toolkit NNI following the [instruction](https://nni.readthedocs.io/en/stable/Tutorial/InstallationLinux.html#installation) 
- [ ] Read the document about how to use NNI for NAS: [Document](https://nni.readthedocs.io/en/stable/nas.html)
- [ ] 
